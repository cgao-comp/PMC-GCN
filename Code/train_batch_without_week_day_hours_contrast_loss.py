# -*- coding: utf-8 -*-
"""
Created on Sat Jan 30 12:46:02 2021

@author: wzhangcd
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module

import math
import numpy as np
import pandas as pd

import sys

sys.path.append('./lib/')
from pkl_process import *
from lib.utils import load_graphdata_channel_my, compute_val_loss_sttn, generate_torch_datasets, load_features

from time import time
import shutil
import argparse
import configparser
from tensorboardX import SummaryWriter
import os

# from Model.ST_Transformer_new import STTransformer # STTN model with linear layer to get positional embedding
# from Model.ST_Transformer_new_sinembedding import STTransformer_sinembedding
# from Model.ST_Transformer_new_sinembedding_with_LapGCN import STTransformer_sinembedding
# from Model.ST_Transformer_new_sinembedding_with_LapGCN_and_Trans import STTransformer_sinembedding
from Model.ST_Transformer_new_sinembedding_with_LapGCN_and_Trans_Contrast_loss import STTransformer_sinembedding
# STTN model with sin()/cos() to get positional embedding, the same as "Attention is all your need"

# %%

if __name__ == '__main__':

    params_path = './Experiment/SH_embed_size64_without_week_day_hour_wth_contrast_loss'  ## Path for saving network parameters
    print('params_path:', params_path)
    # filename = './PEMSD7/V_25_r2_d1_w2_astcgn.npz' ## Data generated by prepareData.py
    # filename = './Data/SH/SH_flow_r2_d1_w2_astcgn.npz'  ## Data generated by prepareData.py
    ### Training Hyparameter
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    DEVICE = device
    print("device:{}".format(device))
    batch_size = 16
    learning_rate = 0.0001
    epochs = 800
    # LH
    feat_name = "./Data/SH/SH_flow.csv"
    # 加载特征矩阵X
    feat = load_features(feat_name)
    seq_len = 12
    pre_len = 4
    split_ratio = 0.8
    normalize = True
    time_len = None
    shuffle = True
    test_dataset, train_dataset, train_x_tensor, train_target_tensor, test_x_tesor, test_target_tensor, max_val = \
        generate_torch_datasets(feat, seq_len, pre_len, time_len, split_ratio, normalize)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # train_loader = train_loader.unsqueeze(1)
    # train_loader = train_loader.permute(0, 3, 1, 2)
    # train_target_tensor = train_target_tensor.permute(0, 2, 1)
    # test_loader = test_loader.unsqueeze(1)
    # test_loader = test_loader.permute(0, 3, 1, 2)
    # test_target_tensor = test_target_tensor.permute(0, 2, 1)
    # LH



    ### Generate Data Loader
    # train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel_my(
    #     filename, num_of_hours, num_of_days, num_of_weeks, DEVICE, batch_size)


    ### Adjacency Matrix Import
    adj_mx = pd.read_csv('./Data/SH/SH_adj.csv', header=None)
    adj_mx = np.array(adj_mx)
    A = adj_mx
    A = torch.Tensor(A)

    ### Training Hyparameter
    in_channels = 1  # Channels of input
    embed_size = 64  # Dimension of hidden embedding features
    # time_num = 288 一天的间隔数
    time_num = 96
    num_layers = 3  # Number of ST Block
    T_dim = 12  # Input length, should be the same as prepareData.py
    output_T_dim = 4  # Output Expected length
    heads = 4  # Number of Heads in MultiHeadAttention
    cheb_K = 2  # Order for Chebyshev Polynomials (Eq 2)
    forward_expansion = 4  # Dimension of Feed Forward Network: embed_size --> embed_size * forward_expansion --> embed_size
    dropout = 0.3

    ### Construct Network
    # net = STTransformer(
    #     A,
    #     in_channels,
    #     embed_size,
    #     time_num,
    #     num_layers,
    #     T_dim,
    #     output_T_dim,
    #     heads,
    #     cheb_K,
    #     forward_expansion,
    #     dropout)

    net = STTransformer_sinembedding(
        A,
        in_channels,
        embed_size,
        time_num,
        num_layers,
        T_dim,
        output_T_dim,
        heads,
        cheb_K,
        device,
        forward_expansion, dropout)

    net.to(device)

    ### Training Process
    #### Load the parameter we have already learnt if start_epoch does not equal to 0
    start_epoch = 0
    if (start_epoch == 0) and (not os.path.exists(params_path)):
        os.makedirs(params_path)
        print('create params directory %s' % (params_path))
    elif (start_epoch == 0) and (os.path.exists(params_path)):
        shutil.rmtree(params_path)
        os.makedirs(params_path)
        print('delete the old one and create params directory %s' % (params_path))
    elif (start_epoch > 0) and (os.path.exists(params_path)):
        print('train from params directory %s' % (params_path))
    else:
        raise SystemExit('Wrong type of model!')

    #### Loss Function Setting
    criterion = nn.MSELoss().to(device)

    # optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
    # LH 是否是因为学习率太高造成的loss值震荡
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.001)
    # optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)
    # LH
    # optimizer = torch.optim.RMSprop(net.parameters(), lr=0.01)
    # criterion = nn.L1Loss().to('cuda:0')

    #### Training Log Set and Print Network, Optimizer
    sw = SummaryWriter(logdir=params_path, flush_secs=5)
    print(net)
    print('Optimizer\'s state_dict:')
    for var_name in optimizer.state_dict():
        print(var_name, '\t', optimizer.state_dict()[var_name])

    global_step = 0
    best_epoch = 0
    best_val_loss = np.inf
    start_time = time()

    alpha = 0.2

    #### Load parameters from files
    if start_epoch > 0:
        params_filename = os.path.join(params_path, 'epoch_%s.params' % start_epoch)
        net.load_state_dict(torch.load(params_filename))
        print('start epoch:', start_epoch)
        print('load weight from: ', params_filename)

    #### train model
    for epoch in range(start_epoch, epochs):
        training_loss_all = 0.0
        ##### Parameter Saving
        params_filename = os.path.join(params_path, 'epoch_%s.params' % epoch)
        ##### Evaluate on Validation Set
        # No Value
        # val_loss = compute_val_loss_sttn(net, val_loader, criterion, sw, epoch)
        # if val_loss < best_val_loss:
        #     best_val_loss = val_loss
        #     best_epoch = epoch
            # torch.save(net.state_dict(), params_filename)
            # print('save parameters to file: %s' % params_filename)


        net.train()  # ensure dropout layers are in train mode
        for batch_index, batch_data in enumerate(train_loader):

            encoder_inputs, labels = batch_data
            encoder_inputs = encoder_inputs.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            # outputs = net(encoder_inputs.permute(0, 2, 1, 3))
            # loss = criterion(outputs, labels)
            # 输出的值有两个，两个子模型都有dropout，计算两个值的loss，以及标签和预测值的loss
            outputs_dropout1 = net(encoder_inputs.permute(0, 2, 1, 3))
            loss_dropout1 = criterion(outputs_dropout1, labels)
            outputs_dropout2 = net(encoder_inputs.permute(0, 2, 1, 3))
            loss_dropout2 = criterion(outputs_dropout2, labels)
            loss_result = criterion(outputs_dropout1, outputs_dropout2)
            loss = (loss_dropout1 + loss_dropout2) + alpha*loss_result
            loss.backward()
            optimizer.step()    # L2正则化的优化器
            training_loss = loss.item()
            training_loss_all += training_loss
            # # save parameters to file : params filename
            # if training_loss < best_val_loss:
            #     best_val_loss = training_loss
            #     best_epoch = epoch
            #     torch.save(net.state_dict(), params_filename)
            #     print('save parameters to file: %s' % params_filename)
            # LH

            global_step += 1
            sw.add_scalar('training_loss', training_loss, global_step)
            # LH
            print(
                'global step: %s, training loss: %.4f, time: %.2fs' % (global_step, training_loss, time() - start_time))
            # LH
            # if global_step % 38 == 0:
            #     print('global step: %s, training loss: %.4f, time: %.2fs' % (
            #     global_step, training_loss_all, time() - start_time))
        print('global step: %s, training loss: %.4f, time: %.2fs' % (
            global_step, training_loss_all, time() - start_time))
        if training_loss_all < best_val_loss:
            best_val_loss = training_loss_all
            best_epoch = epoch
            torch.save(net.state_dict(), params_filename)
            print('save parameters to file: %s' % params_filename)

        # prediction test
        # 计算test的loss值，以及一些指标，这样就不需要存储训练参数，取得其中的最小值或者平均值

    print('best epoch:', best_epoch)











