# -*- coding: utf-8 -*-
"""
Created on Sat Jan 30 12:46:02 2021
不使用对比学习操作，直接使用PEM-GCN模型，进行两次预测，对比两次预测结果的差异
@author: LH

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module

import math
import numpy as np
import pandas as pd

import sys

sys.path.append('./lib/')
from pkl_process import *
from lib.utils import load_graphdata_channel_my, compute_val_loss_sttn, generate_torch_datasets, load_features

from time import time
import shutil
import argparse
import configparser
from tensorboardX import SummaryWriter
import os

# from Model.ST_Transformer_new import STTransformer # STTN model with linear layer to get positional embedding
# from Model.ST_Transformer_new_sinembedding import STTransformer_sinembedding
# from Model.ST_Transformer_new_sinembedding_with_LapGCN import STTransformer_sinembedding  # ESGCN还混合Transformer
from Model.ST_Transformer_new_sinembedding_with_LapGCN_and_Trans import STTransformer_sinembedding  # ESGCN单独出来
# STTN model with sin()/cos() to get positional embedding, the same as "Attention is all your need"

# %%

if __name__ == '__main__':
    '''all parameter part'''
    seed = 1
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.manual_seed(seed)


    params_path = './Experiment/SH_embed_size64_without_week_day_hour_with_LapGCN_and_Trans'  ## Path for saving network parameters
    print('params_path:', params_path)
    # filename = './PEMSD7/V_25_r2_d1_w2_astcgn.npz' ## Data generated by prepareData.py
    # filename = './Data/SH/SH_flow_r2_d1_w2_astcgn.npz'  ## Data generated by prepareData.py
    ### Training Hyparameter
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    DEVICE = device
    print("device:{}".format(device))
    batch_size = 16
    learning_rate = 0.004
    epochs = 800
    # LH
    feat_name = "./Data/SH/SH_flow.csv"
    # 加载特征矩阵X
    feat = load_features(feat_name)
    seq_len = 12
    pre_len = 4
    split_ratio = 0.8
    normalize = True
    time_len = None
    shuffle = False
    test_dataset, train_dataset, train_x_tensor, train_target_tensor, test_x_tesor, test_target_tensor, max_val = \
        generate_torch_datasets(feat, seq_len, pre_len, time_len, split_ratio, normalize)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # train_loader = train_loader.unsqueeze(1)
    # train_loader = train_loader.permute(0, 3, 1, 2)
    # train_target_tensor = train_target_tensor.permute(0, 2, 1)
    # test_loader = test_loader.unsqueeze(1)
    # test_loader = test_loader.permute(0, 3, 1, 2)
    # test_target_tensor = test_target_tensor.permute(0, 2, 1)
    # LH



    ### Generate Data Loader
    # train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel_my(
    #     filename, num_of_hours, num_of_days, num_of_weeks, DEVICE, batch_size)


    ### Adjacency Matrix Import
    adj_mx = pd.read_csv('./Data/SH/SH_adj.csv', header=None)
    adj_mx = np.array(adj_mx)
    A = adj_mx
    A = torch.Tensor(A)

    ### Training Hyparameter
    in_channels = 1  # Channels of input
    embed_size = 64  # Dimension of hidden embedding features
    # time_num = 288 一天的间隔数
    time_num = 96
    num_layers = 3  # Number of ST Block
    T_dim = 12  # Input length, should be the same as prepareData.py
    output_T_dim = 4  # Output Expected length
    heads = 4  # Number of Heads in MultiHeadAttention 2， 4
    cheb_K = 2  # Order for Chebyshev Polynomials (Eq 2)
    forward_expansion = 4  # Dimension of Feed Forward Network: embed_size --> embed_size * forward_expansion --> embed_size
    dropout = 0

    ### Construct Network
    # net = STTransformer(
    #     A,
    #     in_channels,
    #     embed_size,
    #     time_num,
    #     num_layers,
    #     T_dim,
    #     output_T_dim,
    #     heads,
    #     cheb_K,
    #     forward_expansion,
    #     dropout)

    net = STTransformer_sinembedding(
        A,
        in_channels,
        embed_size,
        time_num,
        num_layers,
        T_dim,
        output_T_dim,
        heads,
        cheb_K,
        device,
        forward_expansion, dropout)

    net.to(device)

    ### Training Process
    #### Load the parameter we have already learnt if start_epoch does not equal to 0
    start_epoch = 0
    if (start_epoch == 0) and (not os.path.exists(params_path)):
        os.makedirs(params_path)
        print('create params directory %s' % (params_path))
    elif (start_epoch == 0) and (os.path.exists(params_path)):
        shutil.rmtree(params_path)
        os.makedirs(params_path)
        print('delete the old one and create params directory %s' % (params_path))
    elif (start_epoch > 0) and (os.path.exists(params_path)):
        print('train from params directory %s' % (params_path))
    else:
        raise SystemExit('Wrong type of model!')

    #### Loss Function Setting
    criterion = nn.MSELoss().to(device)

    # optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
    # LH 是否是因为学习率太高造成的loss值震荡
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
    # optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)
    # LH
    # optimizer = torch.optim.RMSprop(net.parameters(), lr=0.01)
    # criterion = nn.L1Loss().to('cuda:0')

    #### Training Log Set and Print Network, Optimizer
    sw = SummaryWriter(logdir=params_path, flush_secs=5)
    print(net)
    print('Optimizer\'s state_dict:')
    for var_name in optimizer.state_dict():
        print(var_name, '\t', optimizer.state_dict()[var_name])

    global_step = 0
    best_epoch = 0
    best_val_loss = np.inf
    start_time = time()

    #### 查看初始化参数
    weights0_0 =  net.Transformer.encoder.layers[0].ES_GCN.gcn.gcn1.weights
    weights0_1 =  net.Transformer.encoder.layers[0].ES_GCN.gcn.gcn2.weights


    #### Load parameters from files
    if start_epoch > 0:
        params_filename = os.path.join(params_path, 'epoch_%s.params' % start_epoch)
        net.load_state_dict(torch.load(params_filename))
        print('start epoch:', start_epoch)
        print('load weight from: ', params_filename)

    #### train model
    for epoch in range(start_epoch, epochs):
        training_loss_all = 0.0
        ##### Parameter Saving
        params_filename = os.path.join(params_path, 'epoch_%s.params' % epoch)
        ##### Evaluate on Validation Set
        # No Value
        # val_loss = compute_val_loss_sttn(net, val_loader, criterion, sw, epoch)
        # if val_loss < best_val_loss:
        #     best_val_loss = val_loss
        #     best_epoch = epoch
            # torch.save(net.state_dict(), params_filename)
            # print('save parameters to file: %s' % params_filename)


        net.train()  # ensure dropout layers are in train mode
        for batch_index, batch_data in enumerate(train_loader):

            encoder_inputs, labels = batch_data
            encoder_inputs = encoder_inputs.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            outputs = net(encoder_inputs.permute(0, 2, 1, 3).to(device))
            # 恢复原值
            outputs = outputs * max_val
            labels = labels * max_val

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            training_loss = loss.item()
            # LH
            training_loss_all += training_loss
            # # save parameters to file : params filename
            # if training_loss < best_val_loss:
            #     best_val_loss = training_loss
            #     best_epoch = epoch
            #     torch.save(net.state_dict(), params_filename)
            #     print('save parameters to file: %s' % params_filename)
            # LH

            global_step += 1
            sw.add_scalar('training_loss', training_loss, global_step)
            # LH
            print(
                'global step: %s, training loss: %.4f, time: %.2fs' % (global_step, training_loss, time() - start_time))
            # LH
            # if global_step % 38 == 0:
            #     print('global step: %s, training loss: %.4f, time: %.2fs' % (
            #     global_step, training_loss_all, time() - start_time))
        print('global step: %s, training loss: %.4f, time: %.2fs' % (
            global_step, training_loss_all, time() - start_time))
        if training_loss_all < best_val_loss:
            best_val_loss = training_loss_all
            best_epoch = epoch
            torch.save(net.state_dict(), params_filename)
            print('save parameters to file: %s' % params_filename)

    print('best epoch:', best_epoch)
    print("best training_loss_all", best_val_loss)











